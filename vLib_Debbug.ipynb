{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import videoIO as vio\n",
    "import pandas as pd\n",
    "import vplot as vp\n",
    "import vmanipulation as vm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as pyplay\n",
    "\n",
    "df = pd.read_csv('../data.csv')\n",
    "filesDir = '../Database/'\n",
    "df['File'] = df.File.str[-16:]\n",
    "df['File'] = filesDir + df['File']\n",
    "df = df.rename(columns={'Time_duration (mm:ss)': 'Time_duration','Time_start (mm:ss.ms)':'Time_start'})\n",
    "\n",
    "filename = df['File'][73] \n",
    "start = df['Time_start'][73]\n",
    "duration = df['Time_duration'][73]\n",
    "predictor = '/home/paula/Desktop/Personality_recognition/shape_predictor_68_face_landmarks.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getffmpeg():\n",
    "    import platform\n",
    "    OS =  platform.system()\n",
    "    \n",
    "    # linux and MAC OS X\n",
    "    if (OS == \"Linux\" or OS ==\"Darwin\"):\n",
    "        return \"ffmpeg\"\n",
    "    # windows\n",
    "    elif (OS == \"Windows\"):\n",
    "        return \"ffmpeg.exe\"\n",
    "    else:\n",
    "        raise Exception(\"OS not identified\") \n",
    "        \n",
    "def getVideoInfo(filename):\n",
    "    import re\n",
    "    import subprocess as sp\n",
    "    command_info = ['ffprobe', '-show_streams','-i', filename]\n",
    "    pipe = sp.Popen(command_info, stdout=sp.PIPE,stderr=sp.STDOUT)\n",
    "    stream = False\n",
    "    videoInfo = {}\n",
    "    currentChannel = None\n",
    "    for x in pipe.stdout.readlines():\n",
    "        if re.findall('\\\\bSTREAM\\\\b', x.decode()):\n",
    "            stream = True\n",
    "        if re.findall('\\\\b/STREAM\\\\b', x.decode()):\n",
    "            stream = False\n",
    "        if (stream) and (\"STREAM\" not in x.decode()):\n",
    "            key, value = x.decode().strip('\\n').split('=')\n",
    "            if (key==\"index\"):\n",
    "                videoInfo[int(value)] = {} #newchannel\n",
    "                currentChannel = int(value)\n",
    "            elif (currentChannel!= None):\n",
    "                videoInfo[currentChannel][key] = value\n",
    "    return videoInfo\n",
    "\n",
    "def tofloat(string):\n",
    "    try:\n",
    "        return float(string)\n",
    "    except ValueError:\n",
    "        num, denom = string.split('/')\n",
    "        try:\n",
    "            return (float(num)/float(denom))\n",
    "        except ValueError:\n",
    "            print (\"That was no valid number.\")\n",
    "            \n",
    "def splitArray(array, numberOfBlocks, overlapping=0): \n",
    "    \n",
    "    array_size = array.size\n",
    "    step = int(np.ceil(array_size/numberOfBlocks))\n",
    "    \n",
    "    if isinstance(overlapping, float):\n",
    "        overlapping = int(overlapping * step)\n",
    "    \n",
    "    splitArray = np.zeros((numberOfBlocks, step + overlapping))\n",
    "    start = np.arange(0,array_size,step)\n",
    "    stop = np.arange(step+overlapping,array_size,step)\n",
    "    \n",
    "    if len(start)!=len(stop):\n",
    "        aux = np.ones(len(start)-len(stop)) * array_size\n",
    "        stop = (np.append(stop,aux)).astype(int)\n",
    "        \n",
    "    for (i,(s,e)) in enumerate(zip(start, stop)):\n",
    "        splitArray[i,:e-s] = array[s:e]\n",
    "    return splitArray   \n",
    " \n",
    "\n",
    "def splitList(alist, numberOfBlocks, overlapping=0): \n",
    "    \n",
    "    list_size = len(alist)\n",
    "    step = list_size/numberOfBlocks\n",
    "    \n",
    "    if isinstance(overlapping, float):\n",
    "        overlapping = int(overlapping * step)\n",
    "    \n",
    "    start = (np.round(np.arange(0,list_size,step))).astype(int)\n",
    "    stop = (np.round(np.arange(step+overlapping,list_size,step))).astype(int)\n",
    "    splitArray = []\n",
    "    \n",
    "    if len(start)>numberOfBlocks:\n",
    "        start = start[:numberOfBlocks]\n",
    "    \n",
    "    if len(start)!=len(stop):\n",
    "        aux = np.ones(len(start)-len(stop)) * list_size\n",
    "        stop = (np.append(stop,aux)).astype(int)\n",
    "             \n",
    "    for (s,e) in zip(start, stop):\n",
    "        splitArray.append(alist[s:e])    \n",
    "    return splitArray    \n",
    "\n",
    "def splitVideo(filename, start=None, duration=None, mono=True, numberOfBlocks=50, overlapping=0):\n",
    "    import subprocess as sp\n",
    "    import numpy as np\n",
    "    \n",
    "    ffmpeg = getffmpeg()\n",
    "    videoInfo = getVideoInfo(filename)\n",
    "    \n",
    "    if duration == None:\n",
    "        duration = str(videoInfo[0]['duration'])\n",
    "    else:\n",
    "        duration = str(duration)\n",
    "        \n",
    "    if start == None:\n",
    "        start = '0'\n",
    "    else:\n",
    "        start = str(start)\n",
    "    \n",
    "    \n",
    "    H, W = int(videoInfo[0]['height']) , int(videoInfo[0]['width'])\n",
    "    frameRate = tofloat(videoInfo[0]['avg_frame_rate'])\n",
    "    \n",
    "    numberOfFrames = int(np.round(frameRate * float(duration)))\n",
    "    \n",
    "    sampleRate = videoInfo[1]['sample_rate']\n",
    "    numberChannels = videoInfo[1]['channels']\n",
    "    \n",
    "    commandVideo = [ffmpeg, \n",
    "               '-i', filename, \n",
    "               '-ss', start,\n",
    "               '-t', duration,\n",
    "               '-f', 'image2pipe',\n",
    "               '-pix_fmt', 'rgb24',\n",
    "               '-vcodec', 'rawvideo','-']\n",
    "    \n",
    "    commandAudio = [ffmpeg,\n",
    "            '-i', filename,\n",
    "            '-ss', start,   \n",
    "            '-t', duration,\n",
    "            '-f', 's16le',\n",
    "            '-acodec', 'pcm_s16le',\n",
    "            '-ar', sampleRate,\n",
    "            '-ac', numberChannels, \n",
    "            '-']\n",
    "    \n",
    "    pipeVideo = sp.Popen(commandVideo, stdout = sp.PIPE, bufsize=10**8)\n",
    "    pipeAudio = sp.Popen(commandAudio, stdout = sp.PIPE, bufsize=10**8)\n",
    "    \n",
    "    frames=[]\n",
    "    \n",
    "    for it in range(numberOfFrames):\n",
    "        raw_image = pipeVideo.stdout.read(H * W * 3)\n",
    "        image =  np.fromstring(raw_image, dtype=np.uint8)\n",
    "        if (image.size != 0):\n",
    "            image = image.reshape((H,W,3))\n",
    "            frames.append(image)\n",
    "    pipeVideo.kill() \n",
    "    \n",
    "    num = int(int(numberChannels)*int(sampleRate)*float(duration))\n",
    "    raw_audio = pipeAudio.stdout.read(num*2)\n",
    "    audio_array = np.fromstring(raw_audio, dtype=\"int16\")\n",
    "    \n",
    "    if int(numberChannels) > 1:\n",
    "        if len(audio_array) % 2 != 0:\n",
    "            audio_array = audio_array[:-1]\n",
    "        audio_array = audio_array.reshape((len(audio_array)//int(numberChannels),int(numberChannels)))\n",
    "    pipeAudio.kill()\n",
    "    \n",
    "    audioChunks = splitArray(audio_array[:,0],numberOfBlocks, overlapping) \n",
    "    videoChunks = splitList(frames, numberOfBlocks, overlapping)\n",
    "    \n",
    "    return videoChunks, audioChunks \n",
    "       \n",
    "def timeStringToFloat(timestr, timeFormat=[60,1]):\n",
    "    return sum([a*b for a,b in zip(timeFormat, map(float,timestr.split(':')))])\n",
    "\n",
    "#video, audio = splitVideo(filename, start=timeStringToFloat(start), duration=timeStringToFloat(duration), numberOfBlocks=50,overlapping=0.4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgbImg):\n",
    "    import numpy as np\n",
    "    return (np.dot(rgbImg[...,:3], [0.299, 0.587, 0.114])).astype(np.uint8)\n",
    "\n",
    "def getFace(image, plot=False):\n",
    "    import dlib\n",
    "    \n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    if image.ndim > 2:\n",
    "        grayImage = rgb2gray(image[:,:,:3])\n",
    "        faceLocation = detector(grayImage)\n",
    "    else:\n",
    "        faceLocation = detector(image)\n",
    "    \n",
    "    if plot:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import matplotlib.patches as patches\n",
    "        fig, ax = plt.subplots(1)\n",
    "        fig.set_size_inches(24,32)\n",
    "        plt.imshow(image)\n",
    "        for region in faceLocation:\n",
    "            left, top, width, height = region.left(), region.top(), region.width(), region.height()\n",
    "            pat = patches.Rectangle((left, top), width, height, linewidth=1, edgecolor='r',facecolor='none')\n",
    "            ax.add_patch(pat)\n",
    "        plt.show()\n",
    "            \n",
    "    return faceLocation \n",
    "\n",
    "def getFaceLandmarks(image, predictor,plot=False, array=True):\n",
    "    import dlib\n",
    "    import numpy as np\n",
    "\n",
    "    grayImage = rgb2gray(image)\n",
    "    faceLocation = getFace(grayImage)\n",
    "    \n",
    "    predictor = dlib.shape_predictor(predictor)\n",
    "\n",
    "    for region in faceLocation:\n",
    "        landmarks = predictor(grayImage, region)\n",
    "    \n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(1)\n",
    "        plt.imshow(image)\n",
    "        for points in list(landmarks.parts()):\n",
    "            dots = patches.Circle( (points.x, points.y) , 5)\n",
    "            ax.add_patch(dots)\n",
    "        plt.show()    \n",
    "    \n",
    "    if array:\n",
    "        landmarks_array = np.zeros((68,2), int)\n",
    "        for i, points in enumerate (list(landmarks.parts())):\n",
    "            landmarks_array[i,:] = np.array([points.x, points.y])\n",
    "        return landmarks_array\n",
    "    \n",
    "    else:\n",
    "        return landmarks\n",
    "\n",
    "def getCentreOfGravity(faceLandmaks):\n",
    "    return faceLandmaks.mean(axis=0).astype(np.int)    \n",
    "    \n",
    "def plotFaceLandMarks(image, faceLandmarks, extraDots=np.array([])):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "    \n",
    "    fig, ax = plt.subplots(1)\n",
    "    fig.set_size_inches(24,32)\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    if faceLandmarks.ndim == 1:\n",
    "        faceLandmarks = faceLandmarks[np.newaxis]\n",
    "        \n",
    "    for dot in faceLandmarks:\n",
    "        dots = patches.Circle( dot , 5)\n",
    "        ax.add_patch(dots)\n",
    "    \n",
    "    if extraDots.size:\n",
    "        if extraDots.ndim == 1:\n",
    "            extraDots = extraDots[np.newaxis]\n",
    "        for dot in extraDots:\n",
    "            dots = patches.Circle( dot , 10)\n",
    "            patches.Circle.set_color(dots,'r')\n",
    "            ax.add_patch(dots)\n",
    "    plt.show()\n",
    "    \n",
    "def drawLinesOnFace(image, dotArray, dotRef):\n",
    "    x, y = dotRef[0], dotRef[1] \n",
    "    fig, ax = plt.subplots(1)\n",
    "    fig.set_size_inches(24,32)\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    for dot in dotArray:\n",
    "        dots = patches.Circle( dot , 5)\n",
    "        ax.add_patch(dots)\n",
    "    \n",
    "    for dot in dotArray:\n",
    "        dx = dot[0] - x \n",
    "        dy = dot[1] - y\n",
    "        arrow = patches.Arrow(x=x, y=y, dx=dx,dy=dy,width=20)\n",
    "        patches.Arrow.set_alpha(self=arrow,alpha=0.5)\n",
    "        ax.add_patch(arrow)\n",
    "    \n",
    "    ref = patches.Circle( [x,y], 10)\n",
    "    patches.Circle.set_color(ref,'r')\n",
    "    ax.add_patch(ref)\n",
    "    plt.show()    \n",
    "        \n",
    "    \n",
    "def splitShapes(faceLandmarks):\n",
    "    jaw = faceLandmarks[:17]\n",
    "    eyebrows = faceLandmarks[17:27]\n",
    "    nose = faceLandmarks[27:36]\n",
    "    eyes = faceLandmarks[36:48]\n",
    "    mouth = faceLandmarks[48:]\n",
    "    \n",
    "    return jaw, eyebrows, nose, eyes, mouth\n",
    "\n",
    "def displayFrames(listOfFrames):\n",
    "    import matplotlib.pyplot as plt\n",
    "    numberofFrames = len(listOfFrames)\n",
    "    plt.figure(1, figsize=(12,32))\n",
    "    for i, frame in enumerate(listOfFrames):\n",
    "        plt.subplot(numberofFrames//2 + 1,2,i+1)\n",
    "        plt.imshow(frame)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def isLandmarks(landmarks):\n",
    "        try:\n",
    "            if landmarks.shape[0] == 68:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "#img = video[0][0]  \n",
    "#region = getFace(video[0][0], plot=True)\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib.patches as patches\n",
    "#fig, ax = plt.subplots(1)\n",
    "#fig.set_size_inches(24,32)\n",
    "#plt.imshow(img)\n",
    "#left, top, width, height = region.left(), region.top(), region.width(), region.height()\n",
    "#pat = patches.Rectangle((left, top), width, height, linewidth=1, edgecolor='r',facecolor='none')\n",
    "#ax.add_patch(pat)\n",
    "#plt.show()\n",
    "\n",
    "#fl = getFaceLandmarks(img, plot=True)\n",
    "#print(fl)\n",
    "#cog = getCentreOfGravity(fl)\n",
    "#plotFaceLandMarks(img, fl, cog)\n",
    "#drawLinesOnFace(img, fl, cog)\n",
    "#fl=None\n",
    "#print(isLandmarks(fl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMeanFace(faceFrames, split=False):\n",
    "    nvls = 0 # Number of Valid Landmarks Set\n",
    "    meanFace = np.empty((0,2),int)\n",
    "    for aFrame in faceFrames:\n",
    "        faceLandmarks = getFaceLandmarks(aFrame)\n",
    "        if isLandmarks(faceLandmarks):\n",
    "            nvls += 1\n",
    "            if (meanFace.shape[0]==0):\n",
    "                meanFace = np.append(meanFace,faceLandmarks, axis=0)\n",
    "            else:\n",
    "                meanFace += faceLandmarks\n",
    "        else:\n",
    "            continue\n",
    "    meanFace = meanFace // nvls        \n",
    "    if split:\n",
    "        return splitShapes(meanFace)\n",
    "    else:\n",
    "        return meanFace\n",
    "    \n",
    "def plotFace(faceLandmarks,h=None,w=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    #fig.set_size_inches(24,32)\n",
    "    plt.figure(1,figsize=(8,6))\n",
    "    plt.plot(faceLandmarks[:,0],faceLandmarks[:,1],'bo')\n",
    "    xmin, ymin = fl.min(axis=0)\n",
    "    xmax, ymax = fl.max(axis=0)\n",
    "    w = [xmin-200,xmax+200]\n",
    "    h = [ymin-50,ymax+50]\n",
    "    if h: \n",
    "        plt.ylim(h)\n",
    "    if w:\n",
    "        plt.xlim(w)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "     \n",
    "\n",
    "def normalize(array):\n",
    "    return (array - array.min()) /(array.max() - array.min()) \n",
    "\n",
    "def zScore(array):\n",
    "    return (array - array.mean())/array.std()\n",
    "    \n",
    "def getFaceFeatures(faceLandmarks):\n",
    "    centerOfGravity = getCentreOfGravity(faceLandmarks)\n",
    "    magnitude = np.sqrt( ((faceLandmarks - centerOfGravity)**2).sum(axis=1))\n",
    "    magnitude = normalize(magnitude)\n",
    "    delta = faceLandmarks - centerOfGravity\n",
    "    dx, dy = delta[:,0], delta[:,1] \n",
    "    orientation = np.arctan2(dy,dx)\n",
    "    orientation[orientation < 0] += 2 * np.pi\n",
    "    orientation = normalize(orientation)\n",
    "    featureVector = np.empty(magnitude.size + orientation.size)\n",
    "    featureVector[::2] = magnitude\n",
    "    featureVector[1::2]= orientation\n",
    "    \n",
    "    return featureVector\n",
    "        \n",
    "def derivateFaces(videoChunks):\n",
    "    featureMatrix = np.zeros((len(videoChunks), 136)) \n",
    "    progress = 0\n",
    "    step = 100/len(videoChunks)\n",
    "    for i, chunk in enumerate (videoChunks):\n",
    "        meanFaces = getMeanFace(chunk)\n",
    "        if isLandmarks(meanFaces):\n",
    "            featureMatrix[i,:] = getFaceFeatures(meanFaces)\n",
    "        else:\n",
    "            featureMatrix[i,:] = featureMatrix[i-1,:]\n",
    "            print('copy')\n",
    "        progress += step\n",
    "        print (\"progress: {:.2f} %\".format(progress))\n",
    "    return (featureMatrix, featureMatrix[1:,:]-featureMatrix[:-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "labels = np.load('../checkpoints/labels89.npy')\n",
    "featureMatrix = np.load('../checkpoints/fm89.npy')\n",
    "devFeatureMatrix = np.load('../checkpoints/fmd89.npy') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.\n",
      "  2.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.\n",
      "  2.  2.  2.  2.  2.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.\n",
      "  2.  2.  2.  2.  2.  2.  2.  2.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.  2.  2.]\n",
      "(87, 50, 136)\n",
      "(87, 49, 136)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :\n",
      "1  :\n",
      "2  :\n",
      "3  :\n",
      "4  :\n",
      "5  :\n",
      "6  :\n",
      "7  :\n",
      "8  :\n",
      "9  :\n",
      "10  :\n",
      "11  :\n",
      "12  :\n",
      "13  :\n",
      "14  :\n",
      "15  :\n",
      "16  :\n",
      "17  :\n",
      "18  :\n",
      "19  :\n",
      "20  :\n",
      "21  :\n",
      "22  :\n",
      "23  :\n",
      "24  :\n",
      "25  :\n",
      "26  :\n",
      "27  :\n",
      "28  :\n",
      "29  :\n",
      "30  :\n",
      "31  :\n",
      "32  :\n",
      "33  :\n",
      "34  :\n",
      "35  :\n",
      "36  :\n",
      "37  :\n",
      "38  :\n",
      "39  :\n",
      "40  :\n",
      "41  :\n",
      "42  :\n",
      "43  :\n",
      "44  :\n",
      "45  :\n",
      "46  :\n",
      "47  :\n",
      "48  :\n",
      "49  :\n",
      "50  :\n",
      "51  :\n",
      "52  :\n",
      "53  :\n",
      "54  :\n",
      "55  :\n",
      "56  :\n",
      "57  :\n",
      "58  :\n",
      "59  :\n",
      "60  :\n",
      "61  :\n",
      "62  :\n",
      "63  :\n",
      "64  :\n",
      "65  :\n",
      "66  :\n",
      "67  :\n",
      "68  :\n",
      "69  :\n",
      "70  :\n",
      "71  :\n",
      "72  :\n",
      "73  :\n",
      "74  :\n",
      "75  :\n",
      "76  :\n",
      "77  :\n",
      "78  :\n",
      "79  :\n",
      "80  :\n",
      "81  :\n",
      "82  :\n",
      "83  :\n",
      "84  :\n",
      "85  :\n",
      "86  :\n",
      "87  :\n",
      "88  :\n",
      "89  :\n",
      "90  :\n",
      "progress: 2.00 %\n",
      "progress: 4.00 %\n",
      "progress: 6.00 %\n",
      "progress: 8.00 %\n",
      "progress: 10.00 %\n",
      "progress: 12.00 %\n",
      "progress: 14.00 %\n",
      "progress: 16.00 %\n",
      "progress: 18.00 %\n",
      "progress: 20.00 %\n",
      "progress: 22.00 %\n",
      "progress: 24.00 %\n",
      "progress: 26.00 %\n",
      "progress: 28.00 %\n",
      "progress: 30.00 %\n",
      "progress: 32.00 %\n",
      "progress: 34.00 %\n",
      "progress: 36.00 %\n",
      "progress: 38.00 %\n",
      "progress: 40.00 %\n",
      "progress: 42.00 %\n",
      "progress: 44.00 %\n",
      "progress: 46.00 %\n",
      "progress: 48.00 %\n",
      "progress: 50.00 %\n",
      "progress: 52.00 %\n",
      "progress: 54.00 %\n",
      "progress: 56.00 %\n",
      "progress: 58.00 %\n",
      "progress: 60.00 %\n",
      "progress: 62.00 %\n",
      "progress: 64.00 %\n",
      "progress: 66.00 %\n",
      "progress: 68.00 %\n",
      "progress: 70.00 %\n",
      "progress: 72.00 %\n",
      "progress: 74.00 %\n",
      "progress: 76.00 %\n",
      "progress: 78.00 %\n",
      "progress: 80.00 %\n",
      "progress: 82.00 %\n",
      "progress: 84.00 %\n",
      "progress: 86.00 %\n",
      "progress: 88.00 %\n",
      "progress: 90.00 %\n",
      "progress: 92.00 %\n",
      "progress: 94.00 %\n",
      "progress: 96.00 %\n",
      "progress: 98.00 %\n",
      "progress: 100.00 %\n",
      "91  :\n",
      "progress: 2.00 %\n",
      "progress: 4.00 %\n",
      "progress: 6.00 %\n",
      "progress: 8.00 %\n",
      "progress: 10.00 %\n",
      "progress: 12.00 %\n",
      "progress: 14.00 %\n",
      "progress: 16.00 %\n",
      "progress: 18.00 %\n",
      "progress: 20.00 %\n",
      "progress: 22.00 %\n",
      "progress: 24.00 %\n",
      "progress: 26.00 %\n",
      "progress: 28.00 %\n",
      "progress: 30.00 %\n",
      "progress: 32.00 %\n",
      "progress: 34.00 %\n",
      "progress: 36.00 %\n",
      "progress: 38.00 %\n",
      "progress: 40.00 %\n",
      "progress: 42.00 %\n",
      "progress: 44.00 %\n",
      "progress: 46.00 %\n",
      "progress: 48.00 %\n",
      "progress: 50.00 %\n",
      "progress: 52.00 %\n",
      "progress: 54.00 %\n",
      "progress: 56.00 %\n",
      "progress: 58.00 %\n",
      "progress: 60.00 %\n",
      "progress: 62.00 %\n",
      "progress: 64.00 %\n",
      "progress: 66.00 %\n",
      "progress: 68.00 %\n",
      "progress: 70.00 %\n",
      "progress: 72.00 %\n",
      "progress: 74.00 %\n",
      "progress: 76.00 %\n",
      "progress: 78.00 %\n",
      "progress: 80.00 %\n",
      "progress: 82.00 %\n",
      "progress: 84.00 %\n",
      "progress: 86.00 %\n",
      "progress: 88.00 %\n",
      "progress: 90.00 %\n",
      "progress: 92.00 %\n",
      "progress: 94.00 %\n",
      "progress: 96.00 %\n",
      "progress: 98.00 %\n",
      "progress: 100.00 %\n",
      "92  :\n",
      "progress: 2.00 %\n",
      "progress: 4.00 %\n",
      "progress: 6.00 %\n",
      "progress: 8.00 %\n",
      "progress: 10.00 %\n",
      "progress: 12.00 %\n",
      "progress: 14.00 %\n",
      "progress: 16.00 %\n",
      "progress: 18.00 %\n",
      "progress: 20.00 %\n",
      "progress: 22.00 %\n",
      "progress: 24.00 %\n",
      "progress: 26.00 %\n",
      "progress: 28.00 %\n",
      "progress: 30.00 %\n",
      "progress: 32.00 %\n",
      "progress: 34.00 %\n",
      "progress: 36.00 %\n",
      "progress: 38.00 %\n",
      "progress: 40.00 %\n",
      "progress: 42.00 %\n",
      "progress: 44.00 %\n",
      "progress: 46.00 %\n",
      "progress: 48.00 %\n",
      "progress: 50.00 %\n",
      "progress: 52.00 %\n",
      "progress: 54.00 %\n",
      "progress: 56.00 %\n",
      "progress: 58.00 %\n",
      "progress: 60.00 %\n",
      "progress: 62.00 %\n",
      "progress: 64.00 %\n",
      "progress: 66.00 %\n",
      "progress: 68.00 %\n",
      "progress: 70.00 %\n",
      "progress: 72.00 %\n",
      "progress: 74.00 %\n",
      "progress: 76.00 %\n",
      "progress: 78.00 %\n",
      "progress: 80.00 %\n",
      "progress: 82.00 %\n",
      "progress: 84.00 %\n",
      "progress: 86.00 %\n",
      "progress: 88.00 %\n",
      "progress: 90.00 %\n",
      "progress: 92.00 %\n",
      "progress: 94.00 %\n",
      "progress: 96.00 %\n",
      "progress: 98.00 %\n",
      "progress: 100.00 %\n"
     ]
    }
   ],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    print(idx,' :')\n",
    "    \n",
    "    if idx <90:\n",
    "        continue\n",
    "    \n",
    "    filename = row['File']\n",
    "    start = row['Time_start']\n",
    "    duration = row['Time_duration']\n",
    "    I = row['Indv']\n",
    "    P = row['Personality']\n",
    "    E = row['Emotion']\n",
    "    \n",
    "    if row['Split'] == 'No':\n",
    "        \n",
    "        video, audio = splitVideo(filename, start=timeStringToFloat(start), duration=timeStringToFloat(duration), \n",
    "                                  numberOfBlocks=50,overlapping=0.4)  \n",
    "        if (len(video)==50):\n",
    "            m, devm = derivateFaces(video)\n",
    "\n",
    "            if P == 'Introvert':\n",
    "                l = 0\n",
    "            elif P == 'Balanced':\n",
    "                l = 1\n",
    "            elif P == 'Extrovert':\n",
    "                l = 2\n",
    "                \n",
    "            labels = np.append(labels,l)\n",
    "            featureMatrix = np.append(featureMatrix,m[np.newaxis,:], axis=0)\n",
    "            devFeatureMatrix = np.append(devFeatureMatrix, devm[np.newaxis,:], axis=0)\n",
    "\n",
    "            np.save('../checkpoints/labels'+str(idx), labels)\n",
    "            np.save('../checkpoints/fm'+str(idx), featureMatrix)\n",
    "            np.save('../checkpoints/fmd'+str(idx), devFeatureMatrix)\n",
    "\n",
    "        #outputFile = '../Videos/'+ str(k).zfill(2) + '_' + P + '_' + E + '_' + str(I) + '.mp4'\n",
    "        #command = ('ffmpeg -y -i ' + filename + ' -ss ' + start + ' -t ' + duration + ' -vcodec copy -acodec aac ' +\n",
    "        #       '-strict -2 ' + outputFile)\n",
    "        #print(command)\n",
    "        #os.system(command)\n",
    "        #k += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = df['File'][0] \n",
    "start = df['Time_start'][0]\n",
    "duration = df['Time_duration'][0]\n",
    "print(filename)\n",
    "print(start)\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = vio.readAudio(\"../Database/579_0006_01.MP4\",start=start,duration=\"00:24.5\",mono=True)\n",
    "A = vm.normalize(A,normRange=[-1,1])\n",
    "A = vm.removeDC(A)\n",
    "pyplay.Audio(data=A,rate=48000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(getVideoInfo(\"../Database/579_0006_01.MP4\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readFrames(filename, start=\"00:00:00\", duration=None, framedrop=1, fd=None):\n",
    "    import platform\n",
    "    import subprocess as sp\n",
    "    import numpy as np\n",
    "    import videoIO as vio\n",
    "    \n",
    "    OS =  platform.system()\n",
    "    if (OS == \"Linux\"):\n",
    "        FFMPEG_BIN = \"ffmpeg\"\n",
    "    elif (OS == \"Windows\"):\n",
    "        FFMPEG_BIN = \"ffmpeg.exe\"\n",
    "    else:\n",
    "        raise Exception(\"OS not identified\")   \n",
    "    \n",
    "    H,W = vio.getVideoResolution(filename)    \n",
    "    fr = vio.getFrameRate(filename)\n",
    "    nframes = int(fr * vio.timeStringToFloat(duration,timeFormat=[60,1]))\n",
    "    if fd == None:\n",
    "        fd = [H,W]\n",
    "    \n",
    "    command = [ FFMPEG_BIN,\n",
    "            '-ss', start, \n",
    "            '-i', filename,\n",
    "            '-t', duration,\n",
    "            '-f', 'image2pipe',\n",
    "            '-pix_fmt', 'rgb24',\n",
    "            '-vcodec', 'rawvideo', '-']\n",
    "    pipe = sp.Popen(command, stdout = sp.PIPE, bufsize=10**8)\n",
    "    \n",
    "    frameList=[]\n",
    "    for f in range(nframes):\n",
    "        raw_image = pipe.stdout.read(H * W * 3)\n",
    "        image =  np.fromstring(raw_image, dtype=np.uint8)\n",
    "        if (image.size != 0):\n",
    "            if (f%framedrop==0):\n",
    "                image = image.reshape((H,W,3))\n",
    "                image = image[H//2-fd[0]//2:H//2+fd[0]//2, W//2-fd[1]//2:W//2+fd[1]//2,:]\n",
    "                frameList.append(image)\n",
    "    pipe.kill()            \n",
    "    return frameList\n",
    "\n",
    "\n",
    "readFrames('../Database/579_0006_01.MP4', start=\"00:00:00\", duration=None, framedrop=1, fd=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotSpectrum(y,Fs):\n",
    "    \"\"\"\n",
    "    Plots a Single-Sided Amplitude Spectrum of y(t)\n",
    "    \"\"\"\n",
    "    n = len(y) # length of the signal\n",
    "    k = np.arange(n)\n",
    "    T = n/Fs\n",
    "    frq = k/T # two sides frequency range\n",
    "    frq = frq[range(n//2)] # one side frequency range\n",
    "    \n",
    "    Y = np.fft.fft(y)/n # fft computing and normalization\n",
    "    Y = Y[range(n//2)]\n",
    "\n",
    "    plt.plot(frq,abs(Y),'r') # plotting the spectrum\n",
    "    plt.xlabel('Freq (Hz)')\n",
    "    plt.ylabel('|Y(Freq)|')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def energy(signal):\n",
    "    return signal**2    \n",
    "\n",
    "def dftWindows(windows):\n",
    "    dftwin = []\n",
    "    for w  in windows:\n",
    "        dftw = np.fft.fft(w)\n",
    "    \n",
    "def gaussian(x, mu, sig):\n",
    "    return np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.)))\n",
    "    \n",
    "def melFrequency(f):\n",
    "    return 1127 * np.log(1 + f/700)\n",
    "\n",
    "def bark(f):\n",
    "    a = 1\n",
    "    return 13 * a * tan(f/1315.8) + 3.5 * a * tan(f/7518)\n",
    "\n",
    "def logFrequency(f):\n",
    "    return np.log2(f/1000)\n",
    "\n",
    "def logAttackTime(attackStart, attackEnd):\n",
    "    return np.log10(attackEnd - attackStart)\n",
    "\n",
    "def temporalIncrease(attackStart, attackEnd, signal, mu = 0, sig = 0.5):\n",
    "    averageEnergy = np.zeros(attackStart.size)\n",
    "    for i, (s,e) in enumerate (zip(attackStart, attackEnd)):\n",
    "        g = gaussian(np.linspace(-0.5, 0.5, e-s), mu, sig)    \n",
    "        averageEnergy[i] = ( (signal[s:e]**2)*g ).mean()\n",
    "    return averageEnergy\n",
    "\n",
    "def temporalCentroid(signal,sampleRate):\n",
    "    t = np.arange(0,signal.size,1)/sampleRate\n",
    "    return sum( (signal**2)*t) / sum ((signal**2)) \n",
    "    \n",
    "\n",
    "def findVoicedSegmants(signal, t, hold = 0):\n",
    "    tVal = signal.max() * t\n",
    "    signVariationArray = np.diff(np.sign(signal - tVal))\n",
    "    (signVariationIdx,) = np.nonzero(signVariationArray)\n",
    "    signVariationIdx = signVariationIdx.astype(float)\n",
    "    \n",
    "    signVariationIdx = np.insert(signVariationIdx,-1,float('inf'))\n",
    "    signVariationIdx[-2:] = signVariationIdx[-1],signVariationIdx[-2]\n",
    "    \n",
    "    shiftIdx = np.zeros_like(signVariationIdx)\n",
    "    shiftIdx[0] = float('-inf')\n",
    "    shiftIdx[1:] = signVariationIdx[:-1]\n",
    "    \n",
    "    endSegments = shiftIdx[(abs(shiftIdx  - signVariationIdx)>hold)][1:]\n",
    "    startSegments = signVariationIdx[(abs(shiftIdx  - signVariationIdx)>hold)][:-1]\n",
    "    \n",
    "    return (np.vstack((startSegments, endSegments)) ).astype(int)\n",
    "\n",
    "def signalAutoCorrelation(signal, it_limit=None):\n",
    "    signalLength = signal.size\n",
    "    \n",
    "    if it_limit == None:\n",
    "        it_limit = signalLength\n",
    "        \n",
    "    signalCorrelation = np.zeros(it_limit)\n",
    "    \n",
    "    for i in range(it_limit):\n",
    "        signalCorrelation[i] = sum(signal[0:signalLength-i] * signal[i:]) \n",
    "    return signalCorrelation\n",
    "\n",
    "\n",
    "def energyVoicedSegments(signal, t, hold = 0):\n",
    "    vs = findVoicedSegmants(signal, t, hold)\n",
    "    energyVoice = 0\n",
    "    for (s,e) in zip(attackStart, attackEnd):\n",
    "        energyVoice += sum(energy(signal[s:e]))\n",
    "    return energyVoice\n",
    "    \n",
    "def energyUnvoicedSegments(signal, t, hold = 0):\n",
    "    return sum(energy(signal)) - energyVoicedSegments(signal,t,hold)\n",
    "\n",
    "\n",
    "def spectralCentroid(signal, sampleRate):\n",
    "    \n",
    "    n = len(signal)\n",
    "    fft_signal = np.fft.fft(signal) \n",
    "    fft_signal = fft_signal[:n//2]\n",
    "    frequency = np.linspace(0,sampleRate,n)\n",
    "    frequency = frequency[:n//2] \n",
    "    return sum(frequency * abs(fft_signal)) / sum(abs(fft_signal))  \n",
    "\n",
    "def hammingWindow(windowSize):\n",
    "    return 0.54-0.46*np.cos(2*np.pi*(np.arange(windowSize))/windowSize)\n",
    "\n",
    "def STFT(signal, windowSize, overlapping=0):\n",
    "    n = len(signal)\n",
    "    padding = windowSize//2\n",
    "    numberOfWindows = (np.floor( n / (windowSize - overlapping) ) ).astype(int)\n",
    "    windowPadSize = padding * 2 + windowSize \n",
    "    windows = np.zeros((numberOfWindows, windowPadSize),np.complex)\n",
    "    signal = np.append(signal, np.zeros(windowSize))\n",
    "    for i in range(0, numberOfWindows): \n",
    "        windows[i,padding:-padding] = np.fft.fft(signal[i * (windowSize - overlapping):i * (windowSize - overlapping) + windowSize] \n",
    "                                       * hammingWindow(windowSize) )    \n",
    "    return windows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import six\n",
    "import scipy\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "MAX_MEM_BLOCK = 2**8 * 2**10\n",
    "\n",
    "def get_window(window, Nx, fftbins=True):\n",
    "    if six.callable(window):\n",
    "        return window(Nx)\n",
    "\n",
    "    elif (isinstance(window, (six.string_types, tuple)) or\n",
    "          np.isscalar(window)):\n",
    "        # TODO: if we add custom window functions in librosa, call them here\n",
    "\n",
    "        return scipy.signal.get_window(window, Nx, fftbins=fftbins)\n",
    "\n",
    "    elif isinstance(window, (np.ndarray, list)):\n",
    "        if len(window) == Nx:\n",
    "            return np.asarray(window)\n",
    "\n",
    "        raise ParameterError('Window size mismatch: '\n",
    "                             '{:d} != {:d}'.format(len(window), Nx))\n",
    "    else:\n",
    "        raise ParameterError('Invalid window specification: {}'.format(window))\n",
    "\n",
    "def pad_center(data, size, axis=-1, **kwargs):\n",
    "\n",
    "    kwargs.setdefault('mode', 'constant')\n",
    "\n",
    "    n = data.shape[axis]\n",
    "\n",
    "    lpad = int((size - n) // 2)\n",
    "\n",
    "    lengths = [(0, 0)] * data.ndim\n",
    "    lengths[axis] = (lpad, int(size - n - lpad))\n",
    "\n",
    "    if lpad < 0:\n",
    "        raise ParameterError(('Target size ({:d}) must be '\n",
    "                              'at least input size ({:d})').format(size, n))\n",
    "\n",
    "    return np.pad(data, lengths, **kwargs)\n",
    "\n",
    "def valid_audio(y, mono=True):\n",
    "\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        raise ParameterError('data must be of type numpy.ndarray')\n",
    "\n",
    "    if not np.issubdtype(y.dtype, np.float):\n",
    "        raise ParameterError('data must be floating-point')\n",
    "\n",
    "    if mono and y.ndim != 1:\n",
    "        raise ParameterError('Invalid shape for monophonic audio: '\n",
    "                             'ndim={:d}, shape={}'.format(y.ndim, y.shape))\n",
    "\n",
    "    elif y.ndim > 2 or y.ndim == 0:\n",
    "        raise ParameterError('Audio must have shape (samples,) or (channels, samples). '\n",
    "                             'Received shape={}'.format(y.shape))\n",
    "\n",
    "    if not np.isfinite(y).all():\n",
    "        raise ParameterError('Audio buffer is not finite everywhere')\n",
    "\n",
    "    return True\n",
    "\n",
    "def frame(y, frame_length=2048, hop_length=512):\n",
    "\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        raise ParameterError('Input must be of type numpy.ndarray, '\n",
    "                             'given type(y)={}'.format(type(y)))\n",
    "\n",
    "    if y.ndim != 1:\n",
    "        raise ParameterError('Input must be one-dimensional, '\n",
    "                             'given y.ndim={}'.format(y.ndim))\n",
    "\n",
    "    if len(y) < frame_length:\n",
    "        raise ParameterError('Buffer is too short (n={:d})'\n",
    "                             ' for frame_length={:d}'.format(len(y), frame_length))\n",
    "\n",
    "    if hop_length < 1:\n",
    "        raise ParameterError('Invalid hop_length: {:d}'.format(hop_length))\n",
    "\n",
    "    if not y.flags['C_CONTIGUOUS']:\n",
    "        raise ParameterError('Input buffer must be contiguous.')\n",
    "\n",
    "    # Compute the number of frames that will fit. The end may get truncated.\n",
    "    n_frames = 1 + int((len(y) - frame_length) / hop_length)\n",
    "\n",
    "    # Vertical stride is one sample\n",
    "    # Horizontal stride is `hop_length` samples\n",
    "    y_frames = as_strided(y, shape=(frame_length, n_frames),\n",
    "                          strides=(y.itemsize, hop_length * y.itemsize))\n",
    "    return y_frames        \n",
    "        \n",
    "        \n",
    "def stft(y, n_fft=2048, hop_length=None, win_length=None, window='hann',\n",
    "         center=True, dtype=np.complex64, pad_mode='reflect'):\n",
    "    \n",
    "    # By default, use the entire frame\n",
    "    if win_length is None:\n",
    "        win_length = n_fft\n",
    "\n",
    "    # Set the default hop, if it's not already specified\n",
    "    if hop_length is None:\n",
    "        hop_length = int(win_length // 4)\n",
    "\n",
    "    fft_window = get_window(window, win_length, fftbins=True)\n",
    "\n",
    "    # Pad the window out to n_fft size\n",
    "    fft_window = pad_center(fft_window, n_fft)\n",
    "\n",
    "    # Reshape so that the window can be broadcast\n",
    "    fft_window = fft_window.reshape((-1, 1))\n",
    "\n",
    "    # Check audio is valid\n",
    "    #valid_audio(y)\n",
    "\n",
    "    # Pad the time series so that frames are centered\n",
    "    if center:\n",
    "        y = np.pad(y, int(n_fft // 2), mode=pad_mode)\n",
    "\n",
    "    # Window the time series.\n",
    "    y_frames = frame(y, frame_length=n_fft, hop_length=hop_length)\n",
    "\n",
    "    # Pre-allocate the STFT matrix\n",
    "    stft_matrix = np.empty((int(1 + n_fft // 2), y_frames.shape[1]),\n",
    "                           dtype=dtype,\n",
    "                           order='F')\n",
    "\n",
    "    # how many columns can we fit within MAX_MEM_BLOCK?\n",
    "    n_columns = int(MAX_MEM_BLOCK / (stft_matrix.shape[0] *\n",
    "                                          stft_matrix.itemsize))\n",
    "\n",
    "    for bl_s in range(0, stft_matrix.shape[1], n_columns):\n",
    "        bl_t = min(bl_s + n_columns, stft_matrix.shape[1])\n",
    "\n",
    "        # RFFT and Conjugate here to match phase from DPWE code\n",
    "        stft_matrix[:, bl_s:bl_t] = np.fft.fft(fft_window *\n",
    "                                            y_frames[:, bl_s:bl_t],\n",
    "                                            axis=0)[:stft_matrix.shape[0]]\n",
    "\n",
    "    return stft_matrix\n",
    "\n",
    "M = stft(S)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg \n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "import scipy.misc\n",
    "import dlib\n",
    "import numpy as np\n",
    "\n",
    "im = mpimg.imread('../faces/faces3-0.jpg') \n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('/home/paula/Desktop/Personality_recognition/shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "def rgb2gray(rgbImg):\n",
    "    return np.dot(rgbImg[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "gray = rgb2gray(im).astype(np.uint8)\n",
    "rect = detector(gray,1)\n",
    "\n",
    "for r in rect:\n",
    "    \n",
    "    fig, ax = plt.subplots(1)\n",
    "    #ax.imshow(gray, cmap='gray')\n",
    "    ax.imshow(im)\n",
    "    x = patches.Rectangle((r.left(),r.top()),r.width(),r.height(),linewidth=1,edgecolor='r',facecolor='none')\n",
    "    s = predictor(gray,r)\n",
    "    \n",
    "    for p in list(s.parts()):\n",
    "        print(p)\n",
    "        c = patches.Circle( (p.x, p.y) , 5)\n",
    "        ax.add_patch(c)\n",
    "        \n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import videoIO as vio\n",
    "\n",
    "def saveFrames(filename, frames, region=None):\n",
    "    if region:\n",
    "        L, R, T, B = region   \n",
    "    else:\n",
    "        L, R, T, B = (0,frames[0].shape[0],0,frames[0].shape[1])\n",
    "    \n",
    "    for i, f in enumerate(frames):\n",
    "        \n",
    "        cv2.imwrite(filename + str(i) + \".jpg\", f[L:R,T:B,::-1])\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data.csv')\n",
    "filesDir = '../Database/'\n",
    "df['File'] = df.File.str[-16:]\n",
    "df['File'] = filesDir + df['File']\n",
    "df = df.rename(columns={'Time_duration (mm:ss)': 'Time_duration','Time_start (mm:ss.ms)':'Time_start'})\n",
    "\n",
    "region = (100, 1000, 250, 1550)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    fileDir = \"../midFaces/\"\n",
    "    print(idx, row['File'], row['Time_start'], row['Time_duration'])\n",
    "    frames = vio.readFrames(filename=row['File'], start=row['Time_start'], duration=row['Time_duration'],framedrop=10)\n",
    "    fileDir = fileDir + 'faces' + str(idx) + '-' \n",
    "    print(fileDir)\n",
    "    saveFrames(fileDir, frames, region=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#matplotlib inline\n",
    "import numpy as np\n",
    "plt.imshow(frames[0][100:1000, 250:1550,:])\n",
    "plt.show()\n",
    "#import cv2\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib.image as pmimg\n",
    "#import IPython.display\n",
    "#import pandas as pd\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#import gc\n",
    "#from matplotlib import gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data and getting full file path\n",
    "df = pd.read_csv('../data.csv')\n",
    "filesDir = '../Database/'\n",
    "df['File'] = df.File.str[-16:]\n",
    "df['File'] = filesDir + df['File']\n",
    "df = df.rename(columns={'Time_duration (mm:ss)': 'Time_duration','Time_start (mm:ss.ms)':'Time_start'})\n",
    "#print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = df['File'][0] \n",
    "start = df['Time_start'][0]\n",
    "duration = df['Time_duration'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Audio plot functions\n",
    "         \n",
    "def plotSpectrum(signal,Fs):\n",
    "    \n",
    "    n = len(signal)\n",
    "    \n",
    "    freqs = (np.arange(n) * Fs)/n\n",
    "    freqs = freqs[:n//2]\n",
    "    \n",
    "    Y = fft(signal) \n",
    "    Y = Y[:n//2]\n",
    "    \n",
    "    plt.plot(freqs,abs(Y)) # plotting the spectrum\n",
    "    xlabel('Freq (Hz)')\n",
    "    ylabel('|Y(Freq)|')\n",
    "    \n",
    "plotSpectrum (A,48000)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = readAudio(filename,duration,start,mono=True)\n",
    "A = normalize(A,normRange=[-1,1])\n",
    "A = removeDC(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = 0.012\n",
    "l = 10000\n",
    "test =np.sqrt(EA)\n",
    "s = findVoicedSemants(abs(A),t,l)\n",
    "print(s)\n",
    "print(len(s))\n",
    "plotAudio(test)\n",
    "IPython.display.display(IPython.display.Audio(A, rate=48000))\n",
    "for p in s:\n",
    "    IPython.display.display(IPython.display.Audio(A[p[0]:p[1]], rate=48000))\n",
    "    #seg = np.zeros_like(test)\n",
    "    #seg[p[0]:p[1]] = test[p[0]:p[1]]\n",
    "    #plotAudio(seg,Yrange=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IPython.display.display(IPython.display.Audio(A, rate=48000))\n",
    "IPython.display.display(IPython.display.Audio(A[32761:80299], rate=48000))\n",
    "IPython.display.display(IPython.display.Audio(A[117627:179958], rate=48000))\n",
    "IPython.display.display(IPython.display.Audio(A[225978:265026], rate=48000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EEA = energyEnvelop(A,100)\n",
    "EA = energy(A)\n",
    "plotAudio(A[:50000],title='A')\n",
    "plotAudio(EA[:50000],title='EA')\n",
    "plotAudio(np.sqrt(EA)[:100000],title='srtq EA')\n",
    "plotAudio(EEA[:50000],title='EEA')\n",
    "#EA2 = EA.copy()\n",
    "#EA2[EA2<0.01]=0\n",
    "#plotAudio(EA2[300:850])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A1 = readAudio(filename,duration,start, normRange=[-1,1],mono=True)\n",
    "A2 = readAudio(filename,duration, mono=True)\n",
    "A1 = removeDC(A1)\n",
    "A2 = removeDC(A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotAudio(A1)\n",
    "plotAudio(A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A2 , RA, OA = readAudio('../myFile.mp4',duration)\n",
    "\n",
    "IPython.display.display(IPython.display.Audio(A, rate=48000))\n",
    "#IPython.display.display(IPython.display.Audio(A2, rate=48000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotAudio(A,48000)\n",
    "plotAudio(A2,48000)\n",
    "print(A.size)\n",
    "print(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fa = np.fft.fft(A)\n",
    "\n",
    "x = np.abs(fa)\n",
    "print(x.min())\n",
    "print(x.max())\n",
    "\n",
    "fa[np.abs(fa) < 5]=0\n",
    "\n",
    "out = np.fft.ifft(fa)\n",
    "out = out.real\n",
    "#fpass = np.zeros_like(fa)\n",
    "#perc = 0.001\n",
    "#n = fa.size\n",
    "#cut = int(n*perc)\n",
    "#fpass[cut+1:-cut]=1\n",
    "#ff = fpass * fa\n",
    "#out = np.fft.ifft(ff)\n",
    "#out = out.real\n",
    "\n",
    "plotAudio(A[0:50000])\n",
    "plotAudio(out[0:50000])\n",
    "\n",
    "#print(cut)\n",
    "#print(A[:100])\n",
    "#print(out[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IPython.display.display(IPython.display.Audio(out[:100000], rate=sr))\n",
    "IPython.display.display(IPython.display.Audio(A[:100000], rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B = np.copy(A)\n",
    "#block = 100\n",
    "#tr = 0.00000000000000001\n",
    "#for i in range(block, B.size, block):\n",
    "#    if ( (B[i-block:i+block]<tr).all() ):\n",
    "#        B[i-block:i+block]=0\n",
    "\n",
    "#B[np.abs(B)<0.01]=0\n",
    "B[:32000]=0\n",
    "B[80000:100000]=0\n",
    "plotAudio(A[0:100000])\n",
    "plotAudio(B[0:100000])\n",
    "#plotAudio(out[0:100000])\n",
    "#print(np.abs(A).mean())\n",
    "#print(B[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IPython.display.display(IPython.display.Audio(B[:100000], rate=sr))\n",
    "IPython.display.display(IPython.display.Audio(A[:100000], rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "hog = cv2.HOGDescriptor()\n",
    "im = cv2.imread()\n",
    "print(im)\n",
    "#h = hog.compute(im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('/home/eu/anaconda3/pkgs/opencv3-3.1.0-py36_0/share/OpenCV/haarcascades/haarcascade_frontalface_alt.xml')\n",
    "# eye_cascade = cv2.CascadeClassifier('/home/eu/anaconda3/pkgs/opencv3-3.1.0-py36_0/share/OpenCV/haarcascades/haarcascade_eye.xml')\n",
    "\n",
    "#I = pmimg.imread('./Lenna.png')\n",
    "#I = cv2.imread('./Lenna.png')\n",
    "I = lastImg\n",
    "#if (I.dtype.kind == 'f'):\n",
    "#    I = (I * 255).astype(np.uint8)\n",
    "    \n",
    "#print('I:\\n',I[:5,:5,0])\n",
    "gray = cv2.cvtColor(I, cv2.COLOR_RGB2GRAY)\n",
    "gray = gray[100:,500:1400]\n",
    "Ialt = I[100:,500:1400]\n",
    "#plt.imshow(gray, cmap='gray')\n",
    "#print('gray antes:\\n',gray[:5,:5])\n",
    "#gray = gray.astype(np.uint8)\n",
    "#print('gray depois:\\n',gray[:5,:5])\n",
    "#plt.figure(1)\n",
    "#plt.imshow(gray,cmap='gray')\n",
    "\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "print(faces)\n",
    "(x,y,w,h) = faces[0]\n",
    "roi_I = Ialt[y:y+h, x:x+w,:]\n",
    "plt.figure(1)\n",
    "plt.subplot(121)\n",
    "plt.imshow(Ialt)\n",
    "plt.subplot(122)\n",
    "plt.imshow(roi_I)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shape_to_np(shape, dtype=\"int\"):\n",
    "    # initialize the list of (x, y)-coordinates\n",
    "    coords = np.zeros((68, 2), dtype=dtype)\n",
    " \n",
    "    # loop over the 68 facial landmarks and convert them\n",
    "    # to a 2-tuple of (x, y)-coordinates\n",
    "    for i in range(0, 68):\n",
    "        coords[i] = (shape.part(i).x, shape.part(i).y)\n",
    " \n",
    "    # return the list of (x, y)-coordinates\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rect_to_bb(rect):\n",
    "    # take a bounding predicted by dlib and convert it\n",
    "    # to the format (x, y, w, h) as we would normally do\n",
    "    # with OpenCV\n",
    "    x = rect.left()\n",
    "    y = rect.top()\n",
    "    w = rect.right() - x\n",
    "    h = rect.bottom() - y\n",
    " \n",
    "    # return a tuple of (x, y, w, h)\n",
    "    return (x, y, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import argparse\n",
    "import dlib\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as pmimg\n",
    "\n",
    "#ap = argparse.ArgumentParser()\n",
    "#ap.add_argument(\"-p\", \"--shape-predictor\", required=True,\n",
    "#    help=\"path to facial landmark predictor\")\n",
    "#ap.add_argument(\"-i\", \"--image\", required=True,\n",
    "#    help=\"path to input image\")\n",
    "#args = vars(ap.parse_args())\n",
    "\n",
    "# initialize dlib's face detector (HOG-based) and then create\n",
    "# the facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "#predictor = dlib.shape_predictor(args[\"shape_predictor\"])\n",
    "predictor = dlib.shape_predictor(\"./shape_predictor_68_face_landmarks.dat\")\n",
    "#I = pmimg.imread('./Lenna.png')\n",
    "#I = lastImg\n",
    "Ialt = lastImg[100:,500:1400]\n",
    "#Ialt = pmimg.imread('./Lenna.png')\n",
    "Ir = scipy.misc.imresize(Ialt,(500,500))\n",
    "plt.figure(2)\n",
    "gray = cv2.cvtColor(Ir, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# detect faces in the grayscale image\n",
    "rects = detector(gray, 1)\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "#shape2 = predictor(gray, faces[0])\n",
    "print('rec:', rects)\n",
    "print('faces:', faces)\n",
    "# loop over the face detections\n",
    "for (i, rect) in enumerate(rects):\n",
    "    # determine the facial landmarks for the face region, then\n",
    "    # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "    # array\n",
    "    \n",
    "    shape = predictor(gray, rect)\n",
    "    shape = shape_to_np(shape)\n",
    "    print(shape.shape)\n",
    "    # convert dlib's rectangle to a OpenCV-style bounding box\n",
    "    # [i.e., (x, y, w, h)], then draw the face bounding box\n",
    "    (x, y, w, h) = rect_to_bb(rect)\n",
    "    \n",
    "    x1 = rect.left()\n",
    "    y1 = rect.top()\n",
    "    x2 = rect.right()\n",
    "    y2 = rect.bottom();\n",
    "    \n",
    "    #cv2.rectangle(Ir, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    #cv2.rectangle(Ir, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    \n",
    "    # show the face number\n",
    "    #cv2.putText(Ir, \"Face #{}\".format(i + 1), (x - 10, y - 10),\n",
    "    #    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    " \n",
    "    # loop over the (x, y)-coordinates for the facial landmarks\n",
    "    # and draw them on the image\n",
    "    for (x, y) in shape:\n",
    "        cv2.circle(Ir, (x, y), 1, (0, 0, 255), -1)\n",
    "\n",
    "        \n",
    "plt.figure(1)\n",
    "#plt.subplot(121)\n",
    "plt.imshow(Ir)\n",
    "plt.axis('off')\n",
    "#plt.subplot(122)\n",
    "#plt.imshow(Ir)        \n",
    "\n",
    "# show the output image with the face detections + facial landmarks\n",
    "#cv2.imshow(\"Output\", Ir)\n",
    "#cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
