{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import videoIO as vio\n",
    "import pandas as pd\n",
    "import vplot as vp\n",
    "import vmanipulation as vm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('../data.csv')\n",
    "filesDir = '../Database/'\n",
    "df['File'] = df.File.str[-16:]\n",
    "df['File'] = filesDir + df['File']\n",
    "df = df.rename(columns={'Time_duration (mm:ss)': 'Time_duration','Time_start (mm:ss.ms)':'Time_start'})\n",
    "\n",
    "filename = df['File'][0] \n",
    "start = df['Time_start'][0]\n",
    "duration = df['Time_duration'][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = vio.readAudio(filename, duration, start, mono=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = vm.normalize(A,normRange=[-1,1])\n",
    "A = vm.removeDC(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.plotAudio(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 32)\n"
     ]
    }
   ],
   "source": [
    "def plotSpectrum(y,Fs):\n",
    "    \"\"\"\n",
    "    Plots a Single-Sided Amplitude Spectrum of y(t)\n",
    "    \"\"\"\n",
    "    n = len(y) # length of the signal\n",
    "    k = np.arange(n)\n",
    "    T = n/Fs\n",
    "    frq = k/T # two sides frequency range\n",
    "    frq = frq[range(n//2)] # one side frequency range\n",
    "    \n",
    "    Y = np.fft.fft(y)/n # fft computing and normalization\n",
    "    Y = Y[range(n//2)]\n",
    "\n",
    "    plt.plot(frq,abs(Y),'r') # plotting the spectrum\n",
    "    plt.xlabel('Freq (Hz)')\n",
    "    plt.ylabel('|Y(Freq)|')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def energy(signal):\n",
    "    return signal**2    \n",
    "\n",
    "def dftWindows(windows):\n",
    "    dftwin = []\n",
    "    for w  in windows:\n",
    "        dftw = np.fft.fft(w)\n",
    "    \n",
    "def gaussian(x, mu, sig):\n",
    "    return np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.)))\n",
    "    \n",
    "def melFrequency(f):\n",
    "    return 1127 * np.log(1 + f/700)\n",
    "\n",
    "def bark(f):\n",
    "    a = 1\n",
    "    return 13 * a * tan(f/1315.8) + 3.5 * a * tan(f/7518)\n",
    "\n",
    "def logFrequency(f):\n",
    "    return np.log2(f/1000)\n",
    "\n",
    "def logAttackTime(attackStart, attackEnd):\n",
    "    return np.log10(attackEnd - attackStart)\n",
    "\n",
    "def temporalIncrease(attackStart, attackEnd, signal, mu = 0, sig = 0.5):\n",
    "    averageEnergy = np.zeros(attackStart.size)\n",
    "    for i, (s,e) in enumerate (zip(attackStart, attackEnd)):\n",
    "        g = gaussian(np.linspace(-0.5, 0.5, e-s), mu, sig)    \n",
    "        averageEnergy[i] = ( (signal[s:e]**2)*g ).mean()\n",
    "    return averageEnergy\n",
    "\n",
    "def temporalCentroid(signal,sampleRate):\n",
    "    t = np.arange(0,signal.size,1)/sampleRate\n",
    "    return sum( (signal**2)*t) / sum ((signal**2)) \n",
    "    \n",
    "\n",
    "def findVoicedSegmants(signal, t, hold = 0):\n",
    "    tVal = signal.max() * t\n",
    "    signVariationArray = np.diff(np.sign(signal - tVal))\n",
    "    (signVariationIdx,) = np.nonzero(signVariationArray)\n",
    "    signVariationIdx = signVariationIdx.astype(float)\n",
    "    \n",
    "    signVariationIdx = np.insert(signVariationIdx,-1,float('inf'))\n",
    "    signVariationIdx[-2:] = signVariationIdx[-1],signVariationIdx[-2]\n",
    "    \n",
    "    shiftIdx = np.zeros_like(signVariationIdx)\n",
    "    shiftIdx[0] = float('-inf')\n",
    "    shiftIdx[1:] = signVariationIdx[:-1]\n",
    "    \n",
    "    endSegments = shiftIdx[(abs(shiftIdx  - signVariationIdx)>hold)][1:]\n",
    "    startSegments = signVariationIdx[(abs(shiftIdx  - signVariationIdx)>hold)][:-1]\n",
    "    \n",
    "    return (np.vstack((startSegments, endSegments)) ).astype(int)\n",
    "\n",
    "def signalAutoCorrelation(signal, it_limit=None):\n",
    "    signalLength = signal.size\n",
    "    \n",
    "    if it_limit == None:\n",
    "        it_limit = signalLength\n",
    "        \n",
    "    signalCorrelation = np.zeros(it_limit)\n",
    "    \n",
    "    for i in range(it_limit):\n",
    "        signalCorrelation[i] = sum(signal[0:signalLength-i] * signal[i:]) \n",
    "    return signalCorrelation\n",
    "\n",
    "\n",
    "def energyVoicedSegments(signal, t, hold = 0):\n",
    "    vs = findVoicedSegmants(signal, t, hold)\n",
    "    energyVoice = 0\n",
    "    for (s,e) in zip(attackStart, attackEnd):\n",
    "        energyVoice += sum(energy(signal[s:e]))\n",
    "    return energyVoice\n",
    "    \n",
    "def energyUnvoicedSegments(signal, t, hold = 0):\n",
    "    return sum(energy(signal)) - energyVoicedSegments(signal,t,hold)\n",
    "\n",
    "\n",
    "def spectralCentroid(signal, sampleRate):\n",
    "    \n",
    "    n = len(signal)\n",
    "    fft_signal = np.fft.fft(signal) \n",
    "    fft_signal = fft_signal[:n//2]\n",
    "    frequency = np.linspace(0,sampleRate,n)\n",
    "    frequency = frequency[:n//2] \n",
    "    return sum(frequency * abs(fft_signal)) / sum(abs(fft_signal))  \n",
    "\n",
    "def hammingWindow(windowSize):\n",
    "    return 0.54-0.46*np.cos(2*np.pi*(np.arange(windowSize))/windowSize)\n",
    "\n",
    "def STFT(signal, windowSize, overlapping=0):\n",
    "    n = len(signal)\n",
    "    padding = windowSize//2\n",
    "    numberOfWindows = (np.floor( n / (windowSize - overlapping) ) ).astype(int)\n",
    "    windowPadSize = padding * 2 + windowSize \n",
    "    windows = np.zeros((numberOfWindows, windowPadSize),np.complex)\n",
    "    signal = np.append(signal, np.zeros(windowSize))\n",
    "    for i in range(0, numberOfWindows): \n",
    "        windows[i,padding:-padding] = np.fft.fft(signal[i * (windowSize - overlapping):i * (windowSize - overlapping) + windowSize] \n",
    "                                       * hammingWindow(windowSize) )    \n",
    "    return windows\n",
    "\n",
    "S = np.arange(128)\n",
    "w = STFT(S,16,8)\n",
    "print(w.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'util' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-0048dcb895f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstft_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-0048dcb895f1>\u001b[0m in \u001b[0;36mstft\u001b[0;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;31m# how many columns can we fit within MAX_MEM_BLOCK?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     n_columns = int(util.MAX_MEM_BLOCK / (stft_matrix.shape[0] *\n\u001b[0m\u001b[1;32m    128\u001b[0m                                           stft_matrix.itemsize))\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'util' is not defined"
     ]
    }
   ],
   "source": [
    "import six\n",
    "import scipy\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "MAX_MEM_BLOCK = 2**8 * 2**10\n",
    "\n",
    "def get_window(window, Nx, fftbins=True):\n",
    "    if six.callable(window):\n",
    "        return window(Nx)\n",
    "\n",
    "    elif (isinstance(window, (six.string_types, tuple)) or\n",
    "          np.isscalar(window)):\n",
    "        # TODO: if we add custom window functions in librosa, call them here\n",
    "\n",
    "        return scipy.signal.get_window(window, Nx, fftbins=fftbins)\n",
    "\n",
    "    elif isinstance(window, (np.ndarray, list)):\n",
    "        if len(window) == Nx:\n",
    "            return np.asarray(window)\n",
    "\n",
    "        raise ParameterError('Window size mismatch: '\n",
    "                             '{:d} != {:d}'.format(len(window), Nx))\n",
    "    else:\n",
    "        raise ParameterError('Invalid window specification: {}'.format(window))\n",
    "\n",
    "def pad_center(data, size, axis=-1, **kwargs):\n",
    "\n",
    "    kwargs.setdefault('mode', 'constant')\n",
    "\n",
    "    n = data.shape[axis]\n",
    "\n",
    "    lpad = int((size - n) // 2)\n",
    "\n",
    "    lengths = [(0, 0)] * data.ndim\n",
    "    lengths[axis] = (lpad, int(size - n - lpad))\n",
    "\n",
    "    if lpad < 0:\n",
    "        raise ParameterError(('Target size ({:d}) must be '\n",
    "                              'at least input size ({:d})').format(size, n))\n",
    "\n",
    "    return np.pad(data, lengths, **kwargs)\n",
    "\n",
    "def valid_audio(y, mono=True):\n",
    "\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        raise ParameterError('data must be of type numpy.ndarray')\n",
    "\n",
    "    if not np.issubdtype(y.dtype, np.float):\n",
    "        raise ParameterError('data must be floating-point')\n",
    "\n",
    "    if mono and y.ndim != 1:\n",
    "        raise ParameterError('Invalid shape for monophonic audio: '\n",
    "                             'ndim={:d}, shape={}'.format(y.ndim, y.shape))\n",
    "\n",
    "    elif y.ndim > 2 or y.ndim == 0:\n",
    "        raise ParameterError('Audio must have shape (samples,) or (channels, samples). '\n",
    "                             'Received shape={}'.format(y.shape))\n",
    "\n",
    "    if not np.isfinite(y).all():\n",
    "        raise ParameterError('Audio buffer is not finite everywhere')\n",
    "\n",
    "    return True\n",
    "\n",
    "def frame(y, frame_length=2048, hop_length=512):\n",
    "\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        raise ParameterError('Input must be of type numpy.ndarray, '\n",
    "                             'given type(y)={}'.format(type(y)))\n",
    "\n",
    "    if y.ndim != 1:\n",
    "        raise ParameterError('Input must be one-dimensional, '\n",
    "                             'given y.ndim={}'.format(y.ndim))\n",
    "\n",
    "    if len(y) < frame_length:\n",
    "        raise ParameterError('Buffer is too short (n={:d})'\n",
    "                             ' for frame_length={:d}'.format(len(y), frame_length))\n",
    "\n",
    "    if hop_length < 1:\n",
    "        raise ParameterError('Invalid hop_length: {:d}'.format(hop_length))\n",
    "\n",
    "    if not y.flags['C_CONTIGUOUS']:\n",
    "        raise ParameterError('Input buffer must be contiguous.')\n",
    "\n",
    "    # Compute the number of frames that will fit. The end may get truncated.\n",
    "    n_frames = 1 + int((len(y) - frame_length) / hop_length)\n",
    "\n",
    "    # Vertical stride is one sample\n",
    "    # Horizontal stride is `hop_length` samples\n",
    "    y_frames = as_strided(y, shape=(frame_length, n_frames),\n",
    "                          strides=(y.itemsize, hop_length * y.itemsize))\n",
    "    return y_frames        \n",
    "        \n",
    "        \n",
    "def stft(y, n_fft=2048, hop_length=None, win_length=None, window='hann',\n",
    "         center=True, dtype=np.complex64, pad_mode='reflect'):\n",
    "    \n",
    "    # By default, use the entire frame\n",
    "    if win_length is None:\n",
    "        win_length = n_fft\n",
    "\n",
    "    # Set the default hop, if it's not already specified\n",
    "    if hop_length is None:\n",
    "        hop_length = int(win_length // 4)\n",
    "\n",
    "    fft_window = get_window(window, win_length, fftbins=True)\n",
    "\n",
    "    # Pad the window out to n_fft size\n",
    "    fft_window = pad_center(fft_window, n_fft)\n",
    "\n",
    "    # Reshape so that the window can be broadcast\n",
    "    fft_window = fft_window.reshape((-1, 1))\n",
    "\n",
    "    # Check audio is valid\n",
    "    #valid_audio(y)\n",
    "\n",
    "    # Pad the time series so that frames are centered\n",
    "    if center:\n",
    "        y = np.pad(y, int(n_fft // 2), mode=pad_mode)\n",
    "\n",
    "    # Window the time series.\n",
    "    y_frames = frame(y, frame_length=n_fft, hop_length=hop_length)\n",
    "\n",
    "    # Pre-allocate the STFT matrix\n",
    "    stft_matrix = np.empty((int(1 + n_fft // 2), y_frames.shape[1]),\n",
    "                           dtype=dtype,\n",
    "                           order='F')\n",
    "\n",
    "    # how many columns can we fit within MAX_MEM_BLOCK?\n",
    "    n_columns = int(util.MAX_MEM_BLOCK / (stft_matrix.shape[0] *\n",
    "                                          stft_matrix.itemsize))\n",
    "\n",
    "    for bl_s in range(0, stft_matrix.shape[1], n_columns):\n",
    "        bl_t = min(bl_s + n_columns, stft_matrix.shape[1])\n",
    "\n",
    "        # RFFT and Conjugate here to match phase from DPWE code\n",
    "        stft_matrix[:, bl_s:bl_t] = fft.fft(fft_window *\n",
    "                                            y_frames[:, bl_s:bl_t],\n",
    "                                            axis=0)[:stft_matrix.shape[0]]\n",
    "\n",
    "    return stft_matrix\n",
    "\n",
    "M = stft(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_window' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-255b06cd62e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-95724acf2a3a>\u001b[0m in \u001b[0;36mstft\u001b[0;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mhop_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwin_length\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mfft_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwin_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfftbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Pad the window out to n_fft size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_window' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg \n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "import scipy.misc\n",
    "import dlib\n",
    "import numpy as np\n",
    "\n",
    "im = mpimg.imread('../faces/faces3-0.jpg') \n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('/home/paula/Desktop/Personality_recognition/shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "def rgb2gray(rgbImg):\n",
    "    return np.dot(rgbImg[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "gray = rgb2gray(im).astype(np.uint8)\n",
    "rect = detector(gray,1)\n",
    "\n",
    "for r in rect:\n",
    "    \n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(gray,cmap='gray')\n",
    "    x = patches.Rectangle((r.left(),r.top()),r.width(),r.height(),linewidth=1,edgecolor='r',facecolor='none')\n",
    "    #ax.add_patch(x)\n",
    "    \n",
    "    s = predictor(gray,r)\n",
    "    \n",
    "    for p in list(s.parts()):\n",
    "        print(p)\n",
    "        c = patches.Circle( (p.x, p.y) , 5)\n",
    "        ax.add_patch(c)\n",
    "        \n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "def saveFrames(filename, frames, region=None):\n",
    "    if region:\n",
    "        L, R, T, B = region   \n",
    "    else:\n",
    "        L, R, T, B = (0,frames[0].shape[0],0,frames[0].shape[1])\n",
    "    \n",
    "    for i, f in enumerate(frames):\n",
    "        \n",
    "        cv2.imwrite(filename + str(i) + \".jpg\", f[L:R,T:B,::-1])\n",
    "\n",
    "region = (100, 950, 650, 1250)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    fileDir = \"../faces/\"\n",
    "    print(idx, row['File'], row['Time_start'], row['Time_duration'])\n",
    "    frames = vio.readFrames(filename=row['File'], start=row['Time_start'], duration=row['Time_duration'],framedrop=10)\n",
    "    fileDir = fileDir + 'faces' + str(idx) + '-' \n",
    "    print(fileDir)\n",
    "    saveFrames(fileDir, frames, region=region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matplotlib inline\n",
    "import numpy as np\n",
    "#import cv2\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib.image as pmimg\n",
    "#import IPython.display\n",
    "#import pandas as pd\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#import gc\n",
    "#from matplotlib import gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data and getting full file path\n",
    "df = pd.read_csv('../data.csv')\n",
    "filesDir = '../Database/'\n",
    "df['File'] = df.File.str[-16:]\n",
    "df['File'] = filesDir + df['File']\n",
    "df = df.rename(columns={'Time_duration (mm:ss)': 'Time_duration','Time_start (mm:ss.ms)':'Time_start'})\n",
    "#print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = df['File'][0] \n",
    "start = df['Time_start'][0]\n",
    "duration = df['Time_duration'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Audio plot functions\n",
    "         \n",
    "def plotSpectrum(signal,Fs):\n",
    "    \n",
    "    n = len(signal)\n",
    "    \n",
    "    freqs = (np.arange(n) * Fs)/n\n",
    "    freqs = freqs[:n//2]\n",
    "    \n",
    "    Y = fft(signal) \n",
    "    Y = Y[:n//2]\n",
    "    \n",
    "    plt.plot(freqs,abs(Y)) # plotting the spectrum\n",
    "    xlabel('Freq (Hz)')\n",
    "    ylabel('|Y(Freq)|')\n",
    "    \n",
    "plotSpectrum (A,48000)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = readAudio(filename,duration,start,mono=True)\n",
    "A = normalize(A,normRange=[-1,1])\n",
    "A = removeDC(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = 0.012\n",
    "l = 10000\n",
    "test =np.sqrt(EA)\n",
    "s = findVoicedSemants(abs(A),t,l)\n",
    "print(s)\n",
    "print(len(s))\n",
    "plotAudio(test)\n",
    "IPython.display.display(IPython.display.Audio(A, rate=48000))\n",
    "for p in s:\n",
    "    IPython.display.display(IPython.display.Audio(A[p[0]:p[1]], rate=48000))\n",
    "    #seg = np.zeros_like(test)\n",
    "    #seg[p[0]:p[1]] = test[p[0]:p[1]]\n",
    "    #plotAudio(seg,Yrange=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IPython.display.display(IPython.display.Audio(A, rate=48000))\n",
    "IPython.display.display(IPython.display.Audio(A[32761:80299], rate=48000))\n",
    "IPython.display.display(IPython.display.Audio(A[117627:179958], rate=48000))\n",
    "IPython.display.display(IPython.display.Audio(A[225978:265026], rate=48000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EEA = energyEnvelop(A,100)\n",
    "EA = energy(A)\n",
    "plotAudio(A[:50000],title='A')\n",
    "plotAudio(EA[:50000],title='EA')\n",
    "plotAudio(np.sqrt(EA)[:100000],title='srtq EA')\n",
    "plotAudio(EEA[:50000],title='EEA')\n",
    "#EA2 = EA.copy()\n",
    "#EA2[EA2<0.01]=0\n",
    "#plotAudio(EA2[300:850])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A1 = readAudio(filename,duration,start, normRange=[-1,1],mono=True)\n",
    "A2 = readAudio(filename,duration, mono=True)\n",
    "A1 = removeDC(A1)\n",
    "A2 = removeDC(A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotAudio(A1)\n",
    "plotAudio(A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A2 , RA, OA = readAudio('../myFile.mp4',duration)\n",
    "\n",
    "IPython.display.display(IPython.display.Audio(A, rate=48000))\n",
    "#IPython.display.display(IPython.display.Audio(A2, rate=48000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotAudio(A,48000)\n",
    "plotAudio(A2,48000)\n",
    "print(A.size)\n",
    "print(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fa = np.fft.fft(A)\n",
    "\n",
    "x = np.abs(fa)\n",
    "print(x.min())\n",
    "print(x.max())\n",
    "\n",
    "fa[np.abs(fa) < 5]=0\n",
    "\n",
    "out = np.fft.ifft(fa)\n",
    "out = out.real\n",
    "#fpass = np.zeros_like(fa)\n",
    "#perc = 0.001\n",
    "#n = fa.size\n",
    "#cut = int(n*perc)\n",
    "#fpass[cut+1:-cut]=1\n",
    "#ff = fpass * fa\n",
    "#out = np.fft.ifft(ff)\n",
    "#out = out.real\n",
    "\n",
    "plotAudio(A[0:50000])\n",
    "plotAudio(out[0:50000])\n",
    "\n",
    "#print(cut)\n",
    "#print(A[:100])\n",
    "#print(out[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IPython.display.display(IPython.display.Audio(out[:100000], rate=sr))\n",
    "IPython.display.display(IPython.display.Audio(A[:100000], rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B = np.copy(A)\n",
    "#block = 100\n",
    "#tr = 0.00000000000000001\n",
    "#for i in range(block, B.size, block):\n",
    "#    if ( (B[i-block:i+block]<tr).all() ):\n",
    "#        B[i-block:i+block]=0\n",
    "\n",
    "#B[np.abs(B)<0.01]=0\n",
    "B[:32000]=0\n",
    "B[80000:100000]=0\n",
    "plotAudio(A[0:100000])\n",
    "plotAudio(B[0:100000])\n",
    "#plotAudio(out[0:100000])\n",
    "#print(np.abs(A).mean())\n",
    "#print(B[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IPython.display.display(IPython.display.Audio(B[:100000], rate=sr))\n",
    "IPython.display.display(IPython.display.Audio(A[:100000], rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "hog = cv2.HOGDescriptor()\n",
    "im = cv2.imread()\n",
    "print(im)\n",
    "#h = hog.compute(im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('/home/eu/anaconda3/pkgs/opencv3-3.1.0-py36_0/share/OpenCV/haarcascades/haarcascade_frontalface_alt.xml')\n",
    "# eye_cascade = cv2.CascadeClassifier('/home/eu/anaconda3/pkgs/opencv3-3.1.0-py36_0/share/OpenCV/haarcascades/haarcascade_eye.xml')\n",
    "\n",
    "#I = pmimg.imread('./Lenna.png')\n",
    "#I = cv2.imread('./Lenna.png')\n",
    "I = lastImg\n",
    "#if (I.dtype.kind == 'f'):\n",
    "#    I = (I * 255).astype(np.uint8)\n",
    "    \n",
    "#print('I:\\n',I[:5,:5,0])\n",
    "gray = cv2.cvtColor(I, cv2.COLOR_RGB2GRAY)\n",
    "gray = gray[100:,500:1400]\n",
    "Ialt = I[100:,500:1400]\n",
    "#plt.imshow(gray, cmap='gray')\n",
    "#print('gray antes:\\n',gray[:5,:5])\n",
    "#gray = gray.astype(np.uint8)\n",
    "#print('gray depois:\\n',gray[:5,:5])\n",
    "#plt.figure(1)\n",
    "#plt.imshow(gray,cmap='gray')\n",
    "\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "print(faces)\n",
    "(x,y,w,h) = faces[0]\n",
    "roi_I = Ialt[y:y+h, x:x+w,:]\n",
    "plt.figure(1)\n",
    "plt.subplot(121)\n",
    "plt.imshow(Ialt)\n",
    "plt.subplot(122)\n",
    "plt.imshow(roi_I)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shape_to_np(shape, dtype=\"int\"):\n",
    "    # initialize the list of (x, y)-coordinates\n",
    "    coords = np.zeros((68, 2), dtype=dtype)\n",
    " \n",
    "    # loop over the 68 facial landmarks and convert them\n",
    "    # to a 2-tuple of (x, y)-coordinates\n",
    "    for i in range(0, 68):\n",
    "        coords[i] = (shape.part(i).x, shape.part(i).y)\n",
    " \n",
    "    # return the list of (x, y)-coordinates\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rect_to_bb(rect):\n",
    "    # take a bounding predicted by dlib and convert it\n",
    "    # to the format (x, y, w, h) as we would normally do\n",
    "    # with OpenCV\n",
    "    x = rect.left()\n",
    "    y = rect.top()\n",
    "    w = rect.right() - x\n",
    "    h = rect.bottom() - y\n",
    " \n",
    "    # return a tuple of (x, y, w, h)\n",
    "    return (x, y, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import argparse\n",
    "import dlib\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as pmimg\n",
    "\n",
    "#ap = argparse.ArgumentParser()\n",
    "#ap.add_argument(\"-p\", \"--shape-predictor\", required=True,\n",
    "#    help=\"path to facial landmark predictor\")\n",
    "#ap.add_argument(\"-i\", \"--image\", required=True,\n",
    "#    help=\"path to input image\")\n",
    "#args = vars(ap.parse_args())\n",
    "\n",
    "# initialize dlib's face detector (HOG-based) and then create\n",
    "# the facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "#predictor = dlib.shape_predictor(args[\"shape_predictor\"])\n",
    "predictor = dlib.shape_predictor(\"./shape_predictor_68_face_landmarks.dat\")\n",
    "#I = pmimg.imread('./Lenna.png')\n",
    "#I = lastImg\n",
    "Ialt = lastImg[100:,500:1400]\n",
    "#Ialt = pmimg.imread('./Lenna.png')\n",
    "Ir = scipy.misc.imresize(Ialt,(500,500))\n",
    "plt.figure(2)\n",
    "gray = cv2.cvtColor(Ir, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# detect faces in the grayscale image\n",
    "rects = detector(gray, 1)\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "#shape2 = predictor(gray, faces[0])\n",
    "print('rec:', rects)\n",
    "print('faces:', faces)\n",
    "# loop over the face detections\n",
    "for (i, rect) in enumerate(rects):\n",
    "    # determine the facial landmarks for the face region, then\n",
    "    # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "    # array\n",
    "    \n",
    "    shape = predictor(gray, rect)\n",
    "    shape = shape_to_np(shape)\n",
    "    print(shape.shape)\n",
    "    # convert dlib's rectangle to a OpenCV-style bounding box\n",
    "    # [i.e., (x, y, w, h)], then draw the face bounding box\n",
    "    (x, y, w, h) = rect_to_bb(rect)\n",
    "    \n",
    "    x1 = rect.left()\n",
    "    y1 = rect.top()\n",
    "    x2 = rect.right()\n",
    "    y2 = rect.bottom();\n",
    "    \n",
    "    #cv2.rectangle(Ir, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    #cv2.rectangle(Ir, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    \n",
    "    # show the face number\n",
    "    #cv2.putText(Ir, \"Face #{}\".format(i + 1), (x - 10, y - 10),\n",
    "    #    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    " \n",
    "    # loop over the (x, y)-coordinates for the facial landmarks\n",
    "    # and draw them on the image\n",
    "    for (x, y) in shape:\n",
    "        cv2.circle(Ir, (x, y), 1, (0, 0, 255), -1)\n",
    "\n",
    "        \n",
    "plt.figure(1)\n",
    "#plt.subplot(121)\n",
    "plt.imshow(Ir)\n",
    "plt.axis('off')\n",
    "#plt.subplot(122)\n",
    "#plt.imshow(Ir)        \n",
    "\n",
    "# show the output image with the face detections + facial landmarks\n",
    "#cv2.imshow(\"Output\", Ir)\n",
    "#cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
